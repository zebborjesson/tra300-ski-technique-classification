{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d3cd0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from get_to_know_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5bb39c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "raw_outputs = Path(\"../data_sync/outputs\")\n",
    "files = list(raw_outputs.glob(\"*.csv\"))\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fbb7fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_columns = ['tt_s', 'Lap', 'Gear', 'ns1:LatitudeDegrees', 'ns1:LongitudeDegrees', 'Sport', 'Variant'] \n",
    "feature_columns_pole_data = ['speed_kmph', 'power_w', 'frequency_ppm', 'thrust_left_ms', 'thrust_right_ms', 'impulse_left_ns', 'impulse_right_ns', 'force_meanl_n', 'force_meanr_n', 'f_tot_mean_n']\n",
    "feature_columns_gnss = ['ns1:AltitudeMeters', 'ns2:Speed', 'ns2:RunCadence', 'ns2:Watts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c557afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['speed_kmph',\n",
    "                    'power_w',\n",
    "                    'frequency_ppm',\n",
    "                    'thrust_left_ms',\n",
    "                    'thrust_right_ms',\n",
    "                    'impulse_left_ns',\n",
    "                    'impulse_right_ns',\n",
    "                    'force_meanl_n',\n",
    "                    'force_meanr_n',\n",
    "                    'f_tot_mean_n',\n",
    "                    'ns2:Speed',\n",
    "                    'ns2:RunCadence',\n",
    "                    'ns2:Watts',\n",
    "                    'ns1:AltitudeMeters'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ced99385",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = info_columns + feature_columns_pole_data + feature_columns_gnss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "119f313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_and_std(train_val_files):\n",
    "    all_files = train_val_files\n",
    "    means = {}\n",
    "    stds = {}\n",
    "    for col in feature_columns_pole_data + feature_columns_gnss:\n",
    "        col_values = []\n",
    "        for file in all_files:\n",
    "            data = load_data(file)\n",
    "            if col in data.columns:\n",
    "                col_values.extend(data[col].dropna().values)\n",
    "        means[col] = np.mean(col_values)\n",
    "        stds[col] = np.std(col_values)\n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d59bb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_files(\n",
    "    files, mean, std,\n",
    "    selected_columns,\n",
    "    feature_columns,\n",
    "    output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Normalize selected columns in multiple CSV files, return DataFrames,\n",
    "    and save normalized versions to another directory.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    normalized_dfs = {}  # store results and return them\n",
    "\n",
    "    for file in files:\n",
    "        df = load_data(file)\n",
    "\n",
    "        # Keep only selected columns\n",
    "        df = df[selected_columns].copy()\n",
    "\n",
    "        # Normalize\n",
    "        for col in feature_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = (df[col] - mean[col]) / std[col]\n",
    "\n",
    "        # Save to new location\n",
    "        out_path = output_dir / Path(file).name\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "        normalized_dfs[file] = df  # store for return\n",
    "\n",
    "    return normalized_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4a4f9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_containing_variant(files, variant='NR'):\n",
    "    selected_files = []\n",
    "    for file in files:\n",
    "        data = load_data(file)\n",
    "        if 'Variant' in data.columns and (data['Variant'] == variant).any():\n",
    "            selected_files.append(file)\n",
    "    return selected_files\n",
    "\n",
    "def extract_test_files_on_skier_ids(files, test_skier_ids):\n",
    "    test_files = []\n",
    "    train_val_files = []\n",
    "    for file in files:\n",
    "        for skier_id in test_skier_ids:\n",
    "            if skier_id in file.name:\n",
    "                test_files.append(file)\n",
    "                break\n",
    "        else:\n",
    "            train_val_files.append(file)\n",
    "    return train_val_files, test_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b4a8c",
   "metadata": {},
   "source": [
    "## Dataset split on skiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc826c",
   "metadata": {},
   "source": [
    "NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "80c0bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"NR_split_on_skiers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "df776c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 files with NR variant found.\n",
      "['BIA24-8_NR_merged_with_gear.csv', 'BIA24-7_NR_merged_with_gear.csv', 'BIA24-18_NR_merged_with_gear.csv', 'BIA24-3_NR_merged_with_gear.csv', 'BIA24-22_NR_merged_with_gear.csv', 'BIA24-4_NR_merged_with_gear.csv', 'BIA24-16_NR_merged_with_gear.csv', 'BIA24-19_NR_merged_with_gear.csv', 'BIA24-9_NR_merged_with_gear.csv', 'BIA24-5_NR_merged_with_gear.csv', 'BIA24-15_NR_merged_with_gear.csv', 'BIA24-20_NR_merged_with_gear.csv']\n"
     ]
    }
   ],
   "source": [
    "NR_files = find_files_containing_variant(files, variant='NR')\n",
    "print(len(NR_files), \"files with NR variant found.\")\n",
    "print([file.name for file in NR_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2f68b865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val files: 10, Test files: 2\n"
     ]
    }
   ],
   "source": [
    "train_val_files, test_files = extract_test_files_on_skier_ids(NR_files, test_skier_ids=['-5', '-22'])\n",
    "print(f\"Train/Val files: {len(train_val_files)}, Test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3e2b07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = extract_mean_and_std(train_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8785468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete.\n"
     ]
    }
   ],
   "source": [
    "normalize_files(train_val_files, mean, std, selected_columns, feature_columns, Path(\"../datasets\") / dataset_name / \"train_val\")\n",
    "normalize_files(test_files, mean, std, selected_columns, feature_columns, Path(\"../datasets\") / dataset_name / \"test\")\n",
    "\n",
    "print(\"Normalization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed88fe",
   "metadata": {},
   "source": [
    "WR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cee07da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"WR_split_on_skiers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f9695309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 files with WR variant found.\n",
      "['BIA24-17_WR_merged_with_gear.csv', 'BIA24-18_WR_merged_with_gear.csv', 'BIA24-7_WR_merged_with_gear.csv', 'BIA24-8_WR_merged_with_gear.csv', 'BIA24-22_WR_merged_with_gear.csv', 'BIA24-3_WR_merged_with_gear.csv', 'BIA24-9_WR_merged_with_gear.csv', 'BIA24-19_WR_merged_with_gear.csv', 'BIA24-16_WR_merged_with_gear.csv', 'BIA24-15_WR_merged_with_gear.csv', 'BIA24-5_WR_merged_with_gear.csv', 'BIA24-20_WR_merged_with_gear.csv']\n"
     ]
    }
   ],
   "source": [
    "WR_files = find_files_containing_variant(files, variant='WR')\n",
    "print(len(WR_files), \"files with WR variant found.\")\n",
    "print([file.name for file in WR_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "04385c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val files: 10, Test files: 2\n"
     ]
    }
   ],
   "source": [
    "train_val_files, test_files = extract_test_files_on_skier_ids(WR_files, test_skier_ids=['-5', '-22'])\n",
    "print(f\"Train/Val files: {len(train_val_files)}, Test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "45aa4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = extract_mean_and_std(train_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "501833b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete.\n"
     ]
    }
   ],
   "source": [
    "normalize_files(train_val_files, mean, std, selected_columns, feature_columns, Path(\"../datasets\") / dataset_name / \"train_val\")\n",
    "normalize_files(test_files, mean, std, selected_columns, feature_columns, Path(\"../datasets\") / dataset_name / \"test\")\n",
    "\n",
    "print(\"Normalization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64451ba0",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9b7fcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"NR_and_WR_split_on_skiers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d1b5414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val files: 20, Test files: 4\n"
     ]
    }
   ],
   "source": [
    "train_val_files, test_files = extract_test_files_on_skier_ids(files, test_skier_ids=['-5', '-22'])\n",
    "print(f\"Train/Val files: {len(train_val_files)}, Test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1ca0f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = extract_mean_and_std(train_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "17a22f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete.\n"
     ]
    }
   ],
   "source": [
    "normalize_files(train_val_files, mean, std, selected_columns, feature_columns, Path(\"../datasets\") / dataset_name / \"train_val\")\n",
    "normalize_files(test_files, mean, std, selected_columns, feature_columns, Path(\"../datasets\") / dataset_name / \"test\")\n",
    "\n",
    "print(\"Normalization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94ea25",
   "metadata": {},
   "source": [
    "# Split 80-20 randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "53849c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge(files, selected_columns):\n",
    "    \"\"\"\n",
    "    Load many CSV files and merge them row-wise into a single DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        df = df[selected_columns]          # keep only needed columns\n",
    "        df[\"source_file\"] = f.name         # optional â€“ track origin\n",
    "        dfs.append(df)\n",
    "\n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    return big_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0012d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_split_rows(df, train_ratio=0.8, random_state=42):\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        train_size=train_ratio,\n",
    "        shuffle=True,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "26c3a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(train_df, feature_columns):\n",
    "    mean = train_df[feature_columns].mean()\n",
    "    std  = train_df[feature_columns].std()\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "aa2ba3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df, mean, std, feature_columns):\n",
    "    df = df.copy()\n",
    "    df[feature_columns] = (df[feature_columns] - mean) / std\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3c518153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6678d",
   "metadata": {},
   "source": [
    "NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1920cb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(\"../datasets/NR_80_20\")\n",
    "\n",
    "# 1. Collect all NR files\n",
    "NR_files = find_files_containing_variant(files, variant=\"NR\")\n",
    "\n",
    "# 2. Merge all files into one big DataFrame\n",
    "big_df = load_and_merge(NR_files, selected_columns)\n",
    "\n",
    "# 3. Split 80/20 row-wise\n",
    "train_df, test_df = train_test_split_rows(big_df, train_ratio=0.8)\n",
    "\n",
    "# 4. Compute normalization parameters from training only\n",
    "mean, std = compute_mean_std(train_df, feature_columns)\n",
    "\n",
    "# 5. Normalize both datasets\n",
    "train_df_norm = normalize_df(train_df, mean, std, feature_columns)\n",
    "test_df_norm  = normalize_df(test_df, mean, std, feature_columns)\n",
    "\n",
    "# 6. Save\n",
    "save_dataset(train_df_norm, dataset_path / \"train_val\" /\"train_val.csv\")\n",
    "save_dataset(test_df_norm,  dataset_path / \"test\" / \"test.csv\")\n",
    "\n",
    "print(\"Dataset created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d3d94",
   "metadata": {},
   "source": [
    "WR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bb49d02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(\"../datasets/WR_80_20\")\n",
    "\n",
    "# 1. Collect all NR files\n",
    "WR_files = find_files_containing_variant(files, variant=\"WR\")\n",
    "\n",
    "# 2. Merge all files into one big DataFrame\n",
    "big_df = load_and_merge(WR_files, selected_columns)\n",
    "\n",
    "# 3. Split 80/20 row-wise\n",
    "train_df, test_df = train_test_split_rows(big_df, train_ratio=0.8)\n",
    "\n",
    "# 4. Compute normalization parameters from training only\n",
    "mean, std = compute_mean_std(train_df, feature_columns)\n",
    "\n",
    "# 5. Normalize both datasets\n",
    "train_df_norm = normalize_df(train_df, mean, std, feature_columns)\n",
    "test_df_norm  = normalize_df(test_df, mean, std, feature_columns)\n",
    "\n",
    "# 6. Save\n",
    "save_dataset(train_df_norm, dataset_path / \"train_val\" /\"train_val.csv\")\n",
    "save_dataset(test_df_norm,  dataset_path / \"test\" / \"test.csv\")\n",
    "\n",
    "print(\"Dataset created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7b8ac",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fa5eb697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(\"../datasets/All_80_20\")\n",
    "\n",
    "# 2. Merge all files into one big DataFrame\n",
    "big_df = load_and_merge(files, selected_columns)\n",
    "\n",
    "# 3. Split 80/20 row-wise\n",
    "train_df, test_df = train_test_split_rows(big_df, train_ratio=0.8)\n",
    "\n",
    "# 4. Compute normalization parameters from training only\n",
    "mean, std = compute_mean_std(train_df, feature_columns)\n",
    "\n",
    "# 5. Normalize both datasets\n",
    "train_df_norm = normalize_df(train_df, mean, std, feature_columns)\n",
    "test_df_norm  = normalize_df(test_df, mean, std, feature_columns)\n",
    "\n",
    "# 6. Save\n",
    "save_dataset(train_df_norm, dataset_path / \"train_val\" /\"train_val.csv\")\n",
    "save_dataset(test_df_norm,  dataset_path / \"test\" / \"test.csv\")\n",
    "\n",
    "print(\"Dataset created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6af28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
