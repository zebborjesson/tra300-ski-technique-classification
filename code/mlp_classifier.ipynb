{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09280445",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b32d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "#              USER CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# 1. File Paths & Selection\n",
    "FOLDER_PATH = \"/home/zebborjesson/Documents/school/tra300_digitalization_in_sports/tra300-ski-technique-classification/data_sync/outputs\"\n",
    "FILE_FILTER = \"NR\" \n",
    "\n",
    "# List of specific filenames to use as the TEST set. \n",
    "# (I added your specific file here, but you can add more like in the LSTM code)\n",
    "TEST_FILES = [\n",
    "    \"BIA24-3_NR_merged_with_gear.csv\"\n",
    "]\n",
    "\n",
    "# 2. Input Features\n",
    "FEATURE_COLS_POLE = [\n",
    "    'speed_kmph', 'power_w', 'frequency_ppm', 'thrust_left_ms', 'thrust_right_ms', \n",
    "    'impulse_left_ns', 'impulse_right_ns', 'force_meanl_n', 'force_meanr_n', 'f_tot_mean_n'\n",
    "]\n",
    "FEATURE_COLS_GNSS = [\n",
    "    'ns1:AltitudeMeters', 'ns2:Speed', 'ns2:RunCadence', 'ns2:Watts'\n",
    "]\n",
    "NEW_RATIO_COLS = [\"force_ratio\", \"thrust_ratio\", \"impulse_ratio\"]\n",
    "\n",
    "INPUT_COLS = FEATURE_COLS_POLE + FEATURE_COLS_GNSS + NEW_RATIO_COLS\n",
    "LABEL_COL = \"Gear\"\n",
    "\n",
    "# 3. Training Hyperparameters\n",
    "HIDDEN_DIM = 128\n",
    "DROPOUT = 0.2\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 200\n",
    "TRANSITION_SAMPLES_TO_REMOVE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044efb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTIONS ---\n",
    "def normalize_label(val: object) -> str:\n",
    "    s = str(val).strip()\n",
    "    m = re.search(r'^-?\\d+(?:\\.\\d+)?', s)\n",
    "    if m:\n",
    "        return f\"{float(m.group(0)):.1f}\"\n",
    "    return s\n",
    "\n",
    "def add_ratios(df):\n",
    "    df = df.copy()\n",
    "    eps = 1e-6\n",
    "    if 'force_meanl_n' in df.columns and 'force_meanr_n' in df.columns:\n",
    "        df[\"force_ratio\"] = df[\"force_meanl_n\"] / (df[\"force_meanl_n\"] + df[\"force_meanr_n\"] + eps)\n",
    "    if 'thrust_left_ms' in df.columns and 'thrust_right_ms' in df.columns:\n",
    "        df[\"thrust_ratio\"] = df[\"thrust_left_ms\"] / (df[\"thrust_left_ms\"] + df[\"thrust_right_ms\"] + eps)\n",
    "    if 'impulse_left_ns' in df.columns and 'impulse_right_ns' in df.columns:\n",
    "        df[\"impulse_ratio\"] = df[\"impulse_left_ns\"] / (df[\"impulse_left_ns\"] + df[\"impulse_right_ns\"] + eps)\n",
    "    return df\n",
    "\n",
    "def filter_unstable_gears(df, label_col, window_size):\n",
    "    if df.empty: return df\n",
    "    gear_changes = df[label_col].ne(df[label_col].shift(1))\n",
    "    change_points = df[gear_changes].index.tolist()\n",
    "    remove_indices = set()\n",
    "    for idx in change_points:\n",
    "        for i in range(idx - window_size, idx + window_size):\n",
    "            remove_indices.add(i)\n",
    "    return df.drop(index=list(remove_indices), errors='ignore')\n",
    "\n",
    "def make_sure_numeric(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "all_csvs = [f for f in os.listdir(FOLDER_PATH) if f.endswith('.csv')]\n",
    "csv_files = [f for f in all_csvs if FILE_FILTER in f] if FILE_FILTER else all_csvs\n",
    "print(f\"Found {len(csv_files)} files matching filter '{FILE_FILTER}'.\")\n",
    "\n",
    "dataframes = {}\n",
    "for file in csv_files:\n",
    "    full_path = os.path.join(FOLDER_PATH, file)\n",
    "    df = pd.read_csv(full_path)\n",
    "    \n",
    "    # Process\n",
    "    df = make_sure_numeric(df, INPUT_COLS)\n",
    "    df = add_ratios(df)\n",
    "    if LABEL_COL in df.columns:\n",
    "        df[LABEL_COL] = df[LABEL_COL].apply(normalize_label)\n",
    "        df = filter_unstable_gears(df, LABEL_COL, TRANSITION_SAMPLES_TO_REMOVE)\n",
    "    \n",
    "    dataframes[file] = df\n",
    "\n",
    "# Verify Test Files exist\n",
    "valid_test_files = [tf for tf in TEST_FILES if tf in dataframes]\n",
    "if len(valid_test_files) != len(TEST_FILES):\n",
    "    print(f\"WARNING: Some test files were not found. Using: {valid_test_files}\")\n",
    "TEST_FILES = valid_test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d65b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 12 files...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_sure_numeric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(full_path)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply helper functions defined earlier\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mmake_sure_numeric\u001b[49m(df, cols_to_numeric)\n\u001b[1;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m add_ratios(df)\n\u001b[1;32m     15\u001b[0m df[label_col] \u001b[38;5;241m=\u001b[39m df[label_col]\u001b[38;5;241m.\u001b[39mapply(normalize_label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_sure_numeric' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Identify keys available for training (All files MINUS test files)\n",
    "train_pool_keys = sorted([k for k in dataframes.keys() if k not in TEST_FILES])\n",
    "\n",
    "# 2. Split files for Train/Val\n",
    "train_keys, val_keys = train_test_split(train_pool_keys, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train files: {len(train_keys)} | Val files: {len(val_keys)} | Test files: {len(TEST_FILES)}\")\n",
    "\n",
    "# 3. Concatenate\n",
    "train_df = pd.concat([dataframes[k] for k in train_keys], ignore_index=True)\n",
    "val_df = pd.concat([dataframes[k] for k in val_keys], ignore_index=True)\n",
    "\n",
    "test_dfs_list = [dataframes[k] for k in TEST_FILES]\n",
    "if test_dfs_list:\n",
    "    test_df = pd.concat(test_dfs_list, ignore_index=True)\n",
    "else:\n",
    "    raise ValueError(\"No valid test files found! Check your configuration.\")\n",
    "\n",
    "# Drop NaNs\n",
    "train_df = train_df.dropna(subset=INPUT_COLS + [LABEL_COL])\n",
    "val_df = val_df.dropna(subset=INPUT_COLS + [LABEL_COL])\n",
    "test_df = test_df.dropna(subset=INPUT_COLS + [LABEL_COL])\n",
    "\n",
    "# 4. Fit Label Encoder\n",
    "all_labels = pd.concat([train_df[LABEL_COL], val_df[LABEL_COL], test_df[LABEL_COL]]).unique()\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(all_labels)\n",
    "print(f\"Classes found: {encoder.classes_}\")\n",
    "\n",
    "# 5. Fit Scaler (TRAIN ONLY)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[INPUT_COLS])\n",
    "print(\"Scaler fitted on training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016d156",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ==== SPLIT STRATEGY ====\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Ensure we list files correctly, excluding the test file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataframes\u001b[49m\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m TEST_FILE])\n\u001b[1;32m      4\u001b[0m test_key \u001b[38;5;241m=\u001b[39m TEST_FILE\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. Split the file names (not the rows)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(df):\n",
    "    X_vals = df[INPUT_COLS].values\n",
    "    y_vals = encoder.transform(df[LABEL_COL].values)\n",
    "    \n",
    "    # Scale features\n",
    "    X_vals = scaler.transform(X_vals)\n",
    "    \n",
    "    # Convert to Tensors\n",
    "    X_t = torch.tensor(X_vals, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y_vals, dtype=torch.long)\n",
    "    \n",
    "    return X_t, y_t\n",
    "\n",
    "print(\"Building tensors...\")\n",
    "X_train, y_train = prepare_dataset(train_df)\n",
    "X_val, y_val = prepare_dataset(val_df)\n",
    "X_test, y_test = prepare_dataset(test_df)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Val shape:   {X_val.shape}\")\n",
    "print(f\"Test shape:  {X_test.shape}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306ef20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---- Encode labels using ALL labels (train + val + test) ----\u001b[39;00m\n\u001b[1;32m      2\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m----> 3\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mtrain_df\u001b[49m[label_col], val_df[label_col], test_df[label_col]], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m label_encoder\u001b[38;5;241m.\u001b[39mfit(all_labels)\n\u001b[1;32m      6\u001b[0m y_train \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(train_df[label_col])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # 3 Hidden Layers (as per your previous config)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "class_counts = torch.bincount(y_train)\n",
    "class_weights = 1.0 / class_counts.float().clamp_min(1.0)\n",
    "class_weights = class_weights * (len(class_counts) / class_weights.sum())\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Initialize Model\n",
    "model = MLP(\n",
    "    in_dim=len(INPUT_COLS),\n",
    "    out_dim=len(encoder.classes_),\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    # Save Best\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Load Best Model\n",
    "if best_model_state:\n",
    "    print(f\"\\nRestoring best model (Val Loss: {best_val_loss:.4f})\")\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title(\"Training History\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fabd397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 12 files...\n",
      "Data loading complete.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "# Evaluation Metrics\n",
    "print(\"\\n--- Test Set Classification Report ---\")\n",
    "print(classification_report(all_labels, all_preds, target_names=encoder.classes_, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(encoder.classes_))\n",
    "plt.xticks(tick_marks, encoder.classes_, rotation=45)\n",
    "plt.yticks(tick_marks, encoder.classes_)\n",
    "\n",
    "# Label the squares\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
